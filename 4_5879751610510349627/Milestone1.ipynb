{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training and test data\n",
    "X_train = np.load('data_train.npy')\n",
    "X_test = np.load('data_test.npy')\n",
    "\n",
    "# Load the training labels\n",
    "y_train = pd.read_csv('label_train.csv')\n",
    "# Extract only the label column from y_train\n",
    "y_train = y_train['label'].values\n",
    "\n",
    "# Load the vocabulary map\n",
    "vocab_map_raw = np.load('vocab_map.npy', allow_pickle=True)\n",
    "\n",
    "# Check structure and format vocab_map as a dictionary\n",
    "vocab_map = {i: term for i, term in enumerate(vocab_map_raw)}\n",
    "\n",
    "\n",
    "# Split training data for validation\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Classifier\n",
    "class NaiveBayes:\n",
    "    def __init__(self, smoothing=1.0):\n",
    "        self.class_priors = {}\n",
    "        self.feature_log_likelihoods = {}\n",
    "        self.smoothing = smoothing  # Adjustable smoothing factor\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Number of documents and number of features\n",
    "        n_documents, n_features = X.shape\n",
    "        \n",
    "        # Calculate class priors\n",
    "        unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "        total_docs = n_documents\n",
    "        self.class_priors = {c: class_counts[i] / total_docs for i, c in enumerate(unique_classes)}\n",
    "        \n",
    "        # Calculate feature likelihoods with smoothing\n",
    "        self.feature_log_likelihoods = {}\n",
    "        for c in unique_classes:\n",
    "            class_docs = X[y == c]\n",
    "            class_feature_sum = np.sum(class_docs, axis=0)\n",
    "            \n",
    "            # Applying smoothing, convert likelihoods to log-space\n",
    "            self.feature_log_likelihoods[c] = np.log((class_feature_sum + self.smoothing) / \n",
    "                                                     (np.sum(class_feature_sum) + self.smoothing * n_features))\n",
    "            \n",
    "    def _conditional_log_prob(self, x, c):\n",
    "        # Calculate the log posterior probability for class c\n",
    "        log_prior = np.log(self.class_priors[c])\n",
    "        log_likelihood = np.sum(x * self.feature_log_likelihoods[c])\n",
    "        return log_prior + log_likelihood\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            posteriors = {c: self._conditional_log_prob(x, c) for c in self.class_priors}\n",
    "            # Choose the class with the highest posterior probability\n",
    "            predictions.append(max(posteriors, key=posteriors.get))\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy with TF-IDF and N-grams: 0.7724137931034483\n",
      "f1 score: 0.7724137931034483\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.81      0.84      1416\n",
      "           1       0.53      0.65      0.59       469\n",
      "\n",
      "    accuracy                           0.77      1885\n",
      "   macro avg       0.71      0.73      0.72      1885\n",
      "weighted avg       0.79      0.77      0.78      1885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_model = NaiveBayes()\n",
    "nb_model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = nb_model.predict(X_val)\n",
    "\n",
    "# Check the accuracy\n",
    "accuracy = np.mean(y_val_pred == y_val)\n",
    "print(f\"Validation Accuracy with TF-IDF and N-grams: {accuracy}\")\n",
    "print(f\"f1 score: {f1_score(y_val, y_val_pred, average='micro')}\")\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
